{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0",
   "metadata": {},
   "source": [
    "# Blackjack Card Detection — YOLO Training Pipeline\n",
    "\n",
    "This notebook:\n",
    "1. Trains a YOLOv5-nano **object detection** model on the synthetic card dataset (10 epochs)\n",
    "2. Evaluates on the test split (precision, recall, mAP, confusion matrix)\n",
    "3. Extracts ground-truth crops (Option B) for per-class analysis\n",
    "4. Exports `best.pt` ready to drop into the blackjack bot\n",
    "\n",
    "**Prerequisites:** `data.yaml` and `train/`, `valid/`, `test/` folders in the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1",
   "metadata": {},
   "source": [
    "## 0 — Imports & GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random, shutil, pathlib, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from ultralytics import YOLO\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# ── GPU selection ────────────────────────────────────────────\n",
    "# Change \"1\" to whichever GPU index is free on your server.\n",
    "# Run `nvidia-smi` in a terminal to check availability.\n",
    "os.environ['CUDA_DEVICE_ORDER']   = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('No GPU detected — training will use CPU (slow but works).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2",
   "metadata": {},
   "source": [
    "## 1 — Verify Dataset Structure\n",
    "\n",
    "Quick sanity checks before training: does the data exist, how many images per split, how many classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths — adjust if your layout differs ───────────────────\n",
    "DATA_YAML  = 'data.yaml'\n",
    "TRAIN_IMGS = './train/images'\n",
    "TRAIN_LBLS = './train/labels'\n",
    "VAL_IMGS   = './valid/images'\n",
    "TEST_IMGS  = './test/images'\n",
    "\n",
    "for p in [TRAIN_IMGS, TRAIN_LBLS, VAL_IMGS, TEST_IMGS, DATA_YAML]:\n",
    "    exists = os.path.exists(p)\n",
    "    print(f\"{'✅' if exists else '❌'}  {p}\")\n",
    "    if not exists:\n",
    "        print(f\"   ⚠ Missing! Fix this before training.\")\n",
    "\n",
    "print()\n",
    "for name, path in [('Train', TRAIN_IMGS), ('Valid', VAL_IMGS), ('Test', TEST_IMGS)]:\n",
    "    n = len(glob.glob(os.path.join(path, '*'))) if os.path.isdir(path) else 0\n",
    "    print(f'{name}: {n:,} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Parse data.yaml to get class names ──────────────────────\n",
    "import yaml\n",
    "\n",
    "with open(DATA_YAML) as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "CLASS_NAMES = data_cfg['names']           # list or dict of class names\n",
    "if isinstance(CLASS_NAMES, dict):         # ultralytics sometimes uses {0: 'name', ...}\n",
    "    CLASS_NAMES = [CLASS_NAMES[k] for k in sorted(CLASS_NAMES.keys())]\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f'{NUM_CLASSES} classes: {CLASS_NAMES[:10]} ... {CLASS_NAMES[-5:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3",
   "metadata": {},
   "source": [
    "## 2 — Preview Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = random.sample(os.listdir(TRAIN_IMGS),\n",
    "                             min(9, len(os.listdir(TRAIN_IMGS))))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "for ax, fname in zip(axes.flat, sample_files):\n",
    "    img = plt.imread(os.path.join(TRAIN_IMGS, fname))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(fname, fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Random Training Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4",
   "metadata": {},
   "source": [
    "## 3 — Train YOLO Detection Model\n",
    "\n",
    "Fine-tune YOLOv5-nano (pre-trained on COCO) on the card dataset.  \n",
    "10 epochs for speed — bump to 50–100 for a serious run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Hyperparameters ─────────────────────────────────────────\n",
    "BASE_MODEL = 'yolov5nu.pt'   # nano — fast. Try 'yolov8s.pt' for better accuracy.\n",
    "EPOCHS     = 10\n",
    "BATCH      = 16\n",
    "IMGSZ      = 416\n",
    "DEVICE     = 0 if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = YOLO(BASE_MODEL)\n",
    "\n",
    "results = model.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=EPOCHS,\n",
    "    batch=BATCH,\n",
    "    imgsz=IMGSZ,\n",
    "    optimizer='auto',\n",
    "    device=DEVICE,\n",
    "    cache=False,\n",
    "    patience=5,          # early stopping if val loss plateaus\n",
    "    save=True,\n",
    "    plots=True,          # auto-generate confusion matrix, PR curves, etc.\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# The best weights are saved automatically\n",
    "TRAIN_DIR = pathlib.Path(results.save_dir)\n",
    "BEST_PT   = TRAIN_DIR / 'weights' / 'best.pt'\n",
    "print(f'\\n✅ Training complete.  Best weights: {BEST_PT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5",
   "metadata": {},
   "source": [
    "## 4 — Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = TRAIN_DIR / 'results.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df.columns = df.columns.str.strip()   # ultralytics pads column names with spaces\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Box loss\n",
    "axes[0].plot(df['epoch'], df['train/box_loss'], label='train')\n",
    "axes[0].plot(df['epoch'], df['val/box_loss'],   label='val')\n",
    "axes[0].set_title('Box Loss');  axes[0].legend()\n",
    "\n",
    "# Classification loss\n",
    "axes[1].plot(df['epoch'], df['train/cls_loss'], label='train')\n",
    "axes[1].plot(df['epoch'], df['val/cls_loss'],   label='val')\n",
    "axes[1].set_title('Classification Loss');  axes[1].legend()\n",
    "\n",
    "# mAP\n",
    "axes[2].plot(df['epoch'], df['metrics/mAP50(B)'],    label='mAP@50')\n",
    "axes[2].plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@50-95')\n",
    "axes[2].set_title('Validation mAP');  axes[2].legend()\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6",
   "metadata": {},
   "source": [
    "## 5 — Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_path = TRAIN_DIR / 'confusion_matrix.png'\n",
    "if cm_path.exists():\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    plt.imshow(Image.open(cm_path))\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix (validation set)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f'Confusion matrix not found at {cm_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7",
   "metadata": {},
   "source": [
    "## 6 — Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = YOLO(str(BEST_PT))\n",
    "\n",
    "test_stats = best_model.val(split='test')\n",
    "\n",
    "print('─' * 40)\n",
    "print(f\"Precision : {test_stats.results_dict['metrics/precision(B)']:.3f}\")\n",
    "print(f\"Recall    : {test_stats.results_dict['metrics/recall(B)']:.3f}\")\n",
    "print(f\"mAP@50    : {test_stats.results_dict['metrics/mAP50(B)']:.3f}\")\n",
    "print(f\"mAP@50-95 : {test_stats.results_dict['metrics/mAP50-95(B)']:.3f}\")\n",
    "print('─' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8",
   "metadata": {},
   "source": [
    "## 7 — Visual Predictions on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions and save annotated images\n",
    "pred_results = best_model.predict(\n",
    "    source=TEST_IMGS,\n",
    "    save=True,\n",
    "    imgsz=IMGSZ,\n",
    "    conf=0.25,\n",
    "    verbose=False,\n",
    ")\n",
    "pred_dir = pathlib.Path(pred_results[0].save_dir)\n",
    "\n",
    "# Display a sample\n",
    "pred_images = [f for f in os.listdir(pred_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "samples = random.sample(pred_images, min(10, len(pred_images)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(22, 9))\n",
    "for ax, fname in zip(axes.flat, samples):\n",
    "    img = Image.open(pred_dir / fname)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(fname, fontsize=7)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Test Predictions (annotated)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9",
   "metadata": {},
   "source": [
    "## 8 — Option B: Extract Ground-Truth Crops for Per-Class Analysis\n",
    "\n",
    "This section reads the YOLO-format `.txt` label files, crops each card from the\n",
    "source images using ground-truth bounding boxes, and saves them into an\n",
    "`ImageFolder` structure (`crops/<split>/<class_name>/`).  \n",
    "\n",
    "Useful for:\n",
    "- Inspecting which classes your detector confuses\n",
    "- Training a standalone classification model later if needed\n",
    "- Counting per-class sample distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_ROOT = pathlib.Path('./crops')\n",
    "\n",
    "def extract_crops(images_dir, labels_dir, split_name, class_names):\n",
    "    \"\"\"\n",
    "    Read YOLO-format label files, crop cards from images using\n",
    "    ground-truth bounding boxes, and save into crops/<split>/<class>/.\n",
    "    \n",
    "    Returns a Counter of {class_name: count}.\n",
    "    \"\"\"\n",
    "    out_root = CROP_ROOT / split_name\n",
    "    counts = Counter()\n",
    "    \n",
    "    img_files = sorted(glob.glob(os.path.join(images_dir, '*')))\n",
    "    \n",
    "    for img_path in img_files:\n",
    "        # Derive label path: .../images/foo.jpg → .../labels/foo.txt\n",
    "        stem = pathlib.Path(img_path).stem\n",
    "        label_path = os.path.join(labels_dir, stem + '.txt')\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        with open(label_path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                \n",
    "                cls_id = int(parts[0])\n",
    "                cx, cy, bw, bh = map(float, parts[1:5])\n",
    "                \n",
    "                # Convert YOLO normalized (cx, cy, w, h) → pixel (x1, y1, x2, y2)\n",
    "                x1 = int((cx - bw / 2) * w)\n",
    "                y1 = int((cy - bh / 2) * h)\n",
    "                x2 = int((cx + bw / 2) * w)\n",
    "                y2 = int((cy + bh / 2) * h)\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(w, x2), min(h, y2)\n",
    "                \n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "                \n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                \n",
    "                cls_name = class_names[cls_id] if cls_id < len(class_names) else f'unknown_{cls_id}'\n",
    "                out_dir = out_root / cls_name\n",
    "                out_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                out_file = out_dir / f'{stem}_{i}.jpg'\n",
    "                cv2.imwrite(str(out_file), crop)\n",
    "                counts[cls_name] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "# ── Run extraction for each split ────────────────────────────\n",
    "splits = [\n",
    "    ('train', './train/images', './train/labels'),\n",
    "    ('valid', './valid/images', './valid/labels'),\n",
    "    ('test',  './test/images',  './test/labels'),\n",
    "]\n",
    "\n",
    "all_counts = {}\n",
    "for split_name, img_dir, lbl_dir in splits:\n",
    "    if os.path.isdir(img_dir) and os.path.isdir(lbl_dir):\n",
    "        print(f'Extracting crops for {split_name}...')\n",
    "        counts = extract_crops(img_dir, lbl_dir, split_name, CLASS_NAMES)\n",
    "        all_counts[split_name] = counts\n",
    "        print(f'  → {sum(counts.values()):,} crops across {len(counts)} classes')\n",
    "    else:\n",
    "        print(f'⚠ Skipping {split_name} — directories not found')\n",
    "\n",
    "print(f'\\nCrops saved to: {CROP_ROOT.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10",
   "metadata": {},
   "source": [
    "### 8b — Class Distribution (from crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train' in all_counts:\n",
    "    train_counts = all_counts['train']\n",
    "    sorted_cls = sorted(train_counts.keys())\n",
    "    vals = [train_counts.get(c, 0) for c in sorted_cls]\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.bar(range(len(sorted_cls)), vals, color='steelblue')\n",
    "    plt.xticks(range(len(sorted_cls)), sorted_cls, rotation=90, fontsize=7)\n",
    "    plt.ylabel('Number of Crops')\n",
    "    plt.title('Training Set — Crops per Class (ground truth)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Flag any imbalances\n",
    "    min_cls = min(train_counts, key=train_counts.get)\n",
    "    max_cls = max(train_counts, key=train_counts.get)\n",
    "    print(f'Fewest samples: {min_cls} ({train_counts[min_cls]})')\n",
    "    print(f'Most samples:   {max_cls} ({train_counts[max_cls]})')\n",
    "    print(f'Ratio: {train_counts[max_cls] / max(1, train_counts[min_cls]):.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11",
   "metadata": {},
   "source": [
    "### 8c — Preview Random Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_files = list(CROP_ROOT.rglob('train/**/*.jpg'))\n",
    "if crop_files:\n",
    "    samples = random.sample(crop_files, min(16, len(crop_files)))\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "    for ax, fp in zip(axes.flat, samples):\n",
    "        img = plt.imread(str(fp))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(fp.parent.name, fontsize=9)  # class name is the folder\n",
    "        ax.axis('off')\n",
    "    plt.suptitle('Random Ground-Truth Crops', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No crop files found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12",
   "metadata": {},
   "source": [
    "## 9 — Per-Class Accuracy (via model predictions on crops)\n",
    "\n",
    "Run the trained **detection** model on individual crops to see which card classes\n",
    "it recognises well and which it struggles with. This is a quick proxy for\n",
    "per-class recall on isolated cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crop_dir = CROP_ROOT / 'test'\n",
    "\n",
    "if test_crop_dir.exists():\n",
    "    correct, total = 0, 0\n",
    "    per_class_correct = Counter()\n",
    "    per_class_total   = Counter()\n",
    "    \n",
    "    for cls_dir in sorted(test_crop_dir.iterdir()):\n",
    "        if not cls_dir.is_dir():\n",
    "            continue\n",
    "        true_cls = cls_dir.name\n",
    "        \n",
    "        for img_path in cls_dir.glob('*.jpg'):\n",
    "            preds = best_model.predict(str(img_path), conf=0.2, verbose=False)\n",
    "            \n",
    "            # Take the highest-confidence detection\n",
    "            if preds and len(preds[0].boxes) > 0:\n",
    "                top_box = preds[0].boxes[0]  # highest conf\n",
    "                pred_cls = best_model.names[int(top_box.cls[0])]\n",
    "            else:\n",
    "                pred_cls = 'NO_DETECTION'\n",
    "            \n",
    "            per_class_total[true_cls] += 1\n",
    "            total += 1\n",
    "            if pred_cls == true_cls:\n",
    "                correct += 1\n",
    "                per_class_correct[true_cls] += 1\n",
    "    \n",
    "    print(f'Overall crop accuracy: {correct}/{total} = {correct/max(1,total):.1%}')\n",
    "    print()\n",
    "    \n",
    "    # Show worst classes\n",
    "    per_class_acc = {}\n",
    "    for cls in sorted(per_class_total.keys()):\n",
    "        acc = per_class_correct[cls] / max(1, per_class_total[cls])\n",
    "        per_class_acc[cls] = acc\n",
    "    \n",
    "    worst = sorted(per_class_acc.items(), key=lambda x: x[1])[:10]\n",
    "    print('Worst 10 classes by crop accuracy:')\n",
    "    for cls, acc in worst:\n",
    "        print(f'  {cls:>5s}: {acc:.0%}  ({per_class_correct[cls]}/{per_class_total[cls]})')\n",
    "else:\n",
    "    print('No test crops found — run section 8 first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13",
   "metadata": {},
   "source": [
    "## 10 — Export `best.pt` for the Blackjack Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best.pt to a convenient location\n",
    "EXPORT_PATH = pathlib.Path('./best.pt')\n",
    "shutil.copy2(BEST_PT, EXPORT_PATH)\n",
    "\n",
    "size_mb = EXPORT_PATH.stat().st_size / 1e6\n",
    "print(f'✅ Exported: {EXPORT_PATH.resolve()}  ({size_mb:.1f} MB)')\n",
    "print()\n",
    "print('To use in the blackjack bot:')\n",
    "print('  1. Copy best.pt into the blackjack-bot/ folder')\n",
    "print('  2. Verify config/settings.py has  MODEL_PATH = \"best.pt\"')\n",
    "print('  3. Run:  python -m bot.main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14",
   "metadata": {},
   "source": [
    "## 11 — Quick Smoke Test\n",
    "\n",
    "Confirm the exported model loads and can predict on a single test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the exported weights fresh (simulates what the bot does)\n",
    "smoke_model = YOLO(str(EXPORT_PATH))\n",
    "\n",
    "test_files = glob.glob(os.path.join(TEST_IMGS, '*'))\n",
    "if test_files:\n",
    "    sample = random.choice(test_files)\n",
    "    res = smoke_model.predict(sample, conf=0.25, verbose=False)\n",
    "    \n",
    "    # Draw results\n",
    "    annotated = res[0].plot()  # returns BGR numpy array with boxes drawn\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Smoke test — {len(res[0].boxes)} detections')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detections\n",
    "    for box in res[0].boxes:\n",
    "        cls_name = smoke_model.names[int(box.cls[0])]\n",
    "        conf = float(box.conf[0])\n",
    "        print(f'  {cls_name:>5s}  conf={conf:.2f}  bbox={box.xyxy[0].tolist()}')\n",
    "    \n",
    "    print(f'\\n✅ Model works. Ready for the bot.')\n",
    "else:\n",
    "    print('No test images found.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {"name": "ipython", "version": 3},
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
